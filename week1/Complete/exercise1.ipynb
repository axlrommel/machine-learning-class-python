{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Topics covered:\n",
    "\n",
    "- Overfitting and Underfitting:\n",
    "    - underfitting: a model that is too simple\n",
    "    - overfitting: a model that is not a good predictor\n",
    "- DummyRegressor\n",
    "- Linear Regression:\n",
    "Is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. (from wikipedia: Linear Regression)\n",
    "\n",
    "- Polynomial Features \n",
    "- Regression Metrics (how good is your model?)\n",
    "    - R2 (R square) error: more common\n",
    "    - Mean Absolute error\n",
    "    - Mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read price of stock over time\n",
    "def getXAndY():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('stock.txt')\n",
    "    \n",
    "    # we need to do a reshape of the data because train_test_split expects a [[]] and the X is just one column []\n",
    "    return(df['time'].values.reshape(-1,1),df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x,y)= getXAndY()   \n",
    "plt.plot(x,y,'ro')\n",
    "plt.ylabel('price')\n",
    "plt.xlabel('time/date')\n",
    "plt.title('Stock price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(Xtr, Xtst, ytr, ytst, Xplot, y_plot, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    tr, = plt.plot(Xtr,ytr,'ro')\n",
    "    tst, = plt.plot(Xtst,ytst,'go')\n",
    "    plt.plot(Xplot,y_plot)\n",
    "    plt.legend([tr, tst], ['Train', 'Test'])\n",
    "    plt.ylabel('price')\n",
    "    plt.xlabel('time/date')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a model that always predict the mean: use DummyRegressor\n",
    "# r2_score (r-squared): it's a linear regression scoring function with a best score = or close to 1. \n",
    "def question1():\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    from sklearn.dummy import DummyRegressor\n",
    "    \n",
    "    (X,y)= getXAndY()\n",
    "    \n",
    "    # train_test_split with the default option assigns 75% of the points \n",
    "    # to train and 25% to test    \n",
    "    # use random_state = 0 (or any number) to make sure your results are repeatable\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    \n",
    "    # returns the mean\n",
    "    dummyReg = DummyRegressor(strategy='mean').fit(X_train, y_train)\n",
    "    \n",
    "    # calculate our predicted values for train and test\n",
    "    y_pred_train = dummyReg.predict(X_train)\n",
    "    y_pred_test = dummyReg.predict(X_test)\n",
    "    \n",
    "    plotResults(X_train, X_test, y_train, y_test, X_train, y_pred_train,'Dummy Regressor')\n",
    "    \n",
    "    #compare the predicted values with the real ones\n",
    "    q1 = r2_score(y_train, y_pred_train)\n",
    "    q2 = r2_score(y_test, y_pred_test)\n",
    "    return (q1,q2)\n",
    "\n",
    "question1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a linear regression with a polynomial of degree = 1 ( a line - simple linear regression)\n",
    "def question2():\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "    \n",
    "    (X,y)= getXAndY()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    \n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = linreg.predict(X_train)\n",
    "    y_pred_test = linreg.predict(X_test)\n",
    "    \n",
    "    plotResults(X_train, X_test, y_train, y_test, X_train, y_pred_train,'LinearRegression')\n",
    "    print(linreg.intercept_, linreg.coef_)\n",
    "    q1 = r2_score(y_train, y_pred_train)\n",
    "    q2 = r2_score(y_test, y_pred_test)\n",
    "    return (q1,q2)\n",
    "\n",
    "question2()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a regression for a polynomial of degree 2 (y = ax^2 + bx + c)\n",
    "# PolynomialFeatures turns the data into a polynomial form\n",
    "# e.g. if we want to turn the data into a polynomial of degree 2:\n",
    "# and we have one feature x0, it will convert it to: x0, x0^2\n",
    "# for two features x1, x2, the data will convert to:  x0, x1, x0^2, x0*x1, x1^2\n",
    "def question3():\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    \n",
    "    (X,y)= getXAndY()\n",
    "    X_poly = PolynomialFeatures(degree=2,include_bias=False).fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=0)\n",
    "    \n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred_train = linreg.predict(X_train)\n",
    "    y_pred_test = linreg.predict(X_test)\n",
    "    y_plot = linreg.predict(X_poly)\n",
    "    \n",
    "    # plot the original points train and test\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, random_state=0)\n",
    "    plotResults(X_train_orig, X_test_orig, y_train_orig, y_test_orig, X, y_plot,'Polynomial deg 2')\n",
    "    \n",
    "    print(linreg.intercept_, linreg.coef_)\n",
    "    q1 = r2_score(y_train, y_pred_train)\n",
    "    q2 = r2_score(y_test, y_pred_test)\n",
    "    return (q1,q2)\n",
    "\n",
    "question3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a linear regression for a polynomial of grade 10\n",
    "# return r2 scores for the training and testing set\n",
    "# change the degree between 3 and 10 what do you see?\n",
    "def question4():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    \n",
    "    (X,y)= getXAndY()\n",
    "    \n",
    "    deg = 8\n",
    "    X_poly = PolynomialFeatures(degree=deg,include_bias=False).fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=0)\n",
    "    \n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred_train = linreg.predict(X_train)\n",
    "    y_pred_test = linreg.predict(X_test)\n",
    "    y_plot = linreg.predict(X_poly)\n",
    "    \n",
    "    # plot the original points train and test\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, random_state=0)\n",
    "    plotResults(X_train_orig, X_test_orig, y_train_orig, y_test_orig, X, y_plot,'Polynomial deg ' + str(deg))\n",
    "    \n",
    "    q1 = r2_score(y_train, y_pred_train)\n",
    "    q2 = r2_score(y_test, y_pred_test)\n",
    "    return (q1,q2)\n",
    "\n",
    "question4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a for loop from degree 2 through 9 for the data\n",
    "# capture the r2 scores for the test and training sets for each iteration\n",
    "# return the degree at which you think it's not overfitting or underfitting (high values for r2 scores\n",
    "# for both train and test sets)\n",
    "# extra credit: plot the r2 scores, in the x axis the training set r2, \n",
    "# in the y axis the r2 results for the test set, and label each of the points\n",
    "\n",
    "def question5():\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "    \n",
    "    (X,y)= getXAndY()\n",
    "    r2_train = []\n",
    "    r2_test = []\n",
    "    \n",
    "    for i in range(2,10):\n",
    "        X_poly = PolynomialFeatures(degree=i,include_bias=False).fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=0)\n",
    "    \n",
    "        linreg = LinearRegression().fit(X_train, y_train)\n",
    "        \n",
    "        q1 = r2_score(y_train, linreg.predict(X_train))\n",
    "        q2 = r2_score(y_test, linreg.predict(X_test))\n",
    "        \n",
    "        r2_train.append(q1)\n",
    "        r2_test.append(q2)\n",
    "    \n",
    "    labels = ['{0}'.format(i) for i in range(2,10)]\n",
    "\n",
    "    plt.title(\"R2 score for Train vs Test data for different degrees of polyn\")\n",
    "    plt.plot (r2_train, r2_test, 'r-')\n",
    "    plt.xlabel(\"Train\")\n",
    "    plt.ylabel(\"Test\")\n",
    "    \n",
    "    for label, i, j in zip(labels, r2_train[:], r2_test[:]):\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(i, j), xytext=(-20, 20),\n",
    "            textcoords='offset points', ha='right', va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "    plt.show()\n",
    "    return (6) # seven is as good but maybe overfitting as new data comes\n",
    "\n",
    "question5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
