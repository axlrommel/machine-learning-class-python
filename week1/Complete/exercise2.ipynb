{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics covered:\n",
    "- LinearRegression\n",
    "- PolynomialFeatures\n",
    "- Ridge\n",
    "- MinMaxScaler\n",
    "- Lasso\n",
    "- SVM (Support Vector Machines), SVR (Support Vector Regressor)\n",
    "- GridSearchCV\n",
    "- Regularization\n",
    "- DecisionTreeRegressor\n",
    "- Tips for when having overfitting and underfitting:\n",
    "\n",
    "MinMaxScaler (or any feature normalization): (https://www.coursera.org/learn/python-machine-learning/lecture/M7yUQ/linear-regression-ridge-lasso-and-polynomial-regression)\n",
    "- The test set must use identical scaling as the training set\n",
    "- Fit the scaler using the training set, then apply the same scaler to the test set\n",
    "- Do not scale the training and test set using different scalers as this will lead to random data\n",
    "- Do not fit the scaler using any of the test data, as this could lead to data leakage.\n",
    "\n",
    "GridSearchCV:\n",
    "It is a must use, it serves two purposes\n",
    "1. you are able to run a set of multiple models with different parameters. After it gets run, it prints out the detailed results so you can select the best model.\n",
    "2. for every run (parameter) it will split the training set into a second training set and a cross validation set, and will do multiple runs alternating the training set and cross validation set (see Kfold). This part is controlled by the parameter 'cv'.\n",
    "For example let's say we are going to run a GridSearchCV on a SVR (Support Vector Machine Regressor) with C = [1,10], and cv = 3.\n",
    "GridSearchCV will run 6 times, 3 times with a C = 1, and 3 times with a C = 10. Now for each C, it will split the training set in 3 random sets (cv parameter), let's call them group A, B and C. The first time it will use A and B as training and C to validate, the next time it will use A and C for training and B for validating, the last time it will use B and C for training and A for validating. The mean of the three runs is the final score for each C.\n",
    "The default cv in scikit is 3, but it is common to use a cv=5 or cv=10.\n",
    "The drawback is that for a large cv, it may take a while to run if we have multiple parameters and a large dataset.\n",
    "\n",
    "When having overfitting (high variance), the model is not a good predictor, here is what to do:\n",
    "- get more training examples\n",
    "- reduce the number of features (but don't use PCA to reduce the number of features)\n",
    "- just adding features is likely going to make things worse\n",
    "- Increase the regularization parameter (in e.g. Lasso and Ridge is called alpha parameter)\n",
    "- If using SVMs (SVR or SVC) decrease C (because C = 1/(regularization parameter))\n",
    "\n",
    "When having underfitting (high bias), the model is too simple, here is what to do:\n",
    "- get more features\n",
    "- add polynomial terms\n",
    "- just getting more training examples is not going to help\n",
    "- Decrease the regularization parameter (in e.g. Lasso and Ridge is called alpha parameter)\n",
    "- If using SVMs (SVR or SVC) increase C (because C = 1/(regularization parameter))\n",
    "\n",
    "What is regularization used for?\n",
    "- In lay terms, we use regularization when we have many features, but we want to reduce the magnitude of their influence in the final model.\n",
    "- The other option is just dropping those features altogether.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the crime dataset found here\n",
    "https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "\n",
    "below are the columns from the dataset:\n",
    "\n",
    "predictive_columns = ['population' 'householdsize' 'racepctblack' 'racePctWhite' 'racePctAsian'\n",
    " 'racePctHisp' 'agePct12t21' 'agePct12t29' 'agePct16t24' 'agePct65up'\n",
    " 'numbUrban' 'pctUrban' 'medIncome' 'pctWWage' 'pctWFarmSelf' 'pctWInvInc'\n",
    " 'pctWSocSec' 'pctWPubAsst' 'pctWRetire' 'medFamInc' 'perCapInc'\n",
    " 'whitePerCap' 'blackPerCap' 'indianPerCap' 'AsianPerCap' 'OtherPerCap'\n",
    " 'HispPerCap' 'NumUnderPov' 'PctPopUnderPov' 'PctLess9thGrade'\n",
    " 'PctNotHSGrad' 'PctBSorMore' 'PctUnemployed' 'PctEmploy' 'PctEmplManu'\n",
    " 'PctEmplProfServ' 'PctOccupManu' 'PctOccupMgmtProf' 'MalePctDivorce'\n",
    " 'MalePctNevMarr' 'FemalePctDiv' 'TotalPctDiv' 'PersPerFam' 'PctFam2Par'\n",
    " 'PctKids2Par' 'PctYoungKids2Par' 'PctTeen2Par' 'PctWorkMomYoungKids'\n",
    " 'PctWorkMom' 'NumKidsBornNeverMar' 'PctKidsBornNeverMar' 'NumImmig'\n",
    " 'PctImmigRecent' 'PctImmigRec5' 'PctImmigRec8' 'PctImmigRec10'\n",
    " 'PctRecentImmig' 'PctRecImmig5' 'PctRecImmig8' 'PctRecImmig10'\n",
    " 'PctSpeakEnglOnly' 'PctNotSpeakEnglWell' 'PctLargHouseFam'\n",
    " 'PctLargHouseOccup' 'PersPerOccupHous' 'PersPerOwnOccHous'\n",
    " 'PersPerRentOccHous' 'PctPersOwnOccup' 'PctPersDenseHous' 'PctHousLess3BR'\n",
    " 'MedNumBR' 'HousVacant' 'PctHousOccup' 'PctHousOwnOcc' 'PctVacantBoarded'\n",
    " 'PctVacMore6Mos' 'MedYrHousBuilt' 'PctHousNoPhone' 'PctWOFullPlumb'\n",
    " 'OwnOccLowQuart' 'OwnOccMedVal' 'OwnOccHiQuart' 'OwnOccQrange' 'RentLowQ'\n",
    " 'RentMedian' 'RentHighQ' 'RentQrange' 'MedRent' 'MedRentPctHousInc'\n",
    " 'MedOwnCostPctInc' 'MedOwnCostPctIncNoMtg' 'NumInShelters' 'NumStreet'\n",
    " 'PctForeignBorn' 'PctBornSameState' 'PctSameHouse85' 'PctSameCity85'\n",
    " 'PctSameState85' 'LemasSwornFT' 'LemasSwFTPerPop' 'LemasSwFTFieldOps'\n",
    " 'LemasSwFTFieldPerPop' 'LemasTotalReq' 'LemasTotReqPerPop'\n",
    " 'PolicReqPerOffic' 'PolicPerPop' 'RacialMatchCommPol' 'PctPolicWhite'\n",
    " 'PctPolicBlack' 'PctPolicHisp' 'PctPolicAsian' 'PctPolicMinor'\n",
    " 'OfficAssgnDrugUnits' 'NumKindsDrugsSeiz' 'PolicAveOTWorked' 'LandArea'\n",
    " 'PopDens' 'PctUsePubTrans' 'PolicCars' 'PolicOperBudg'\n",
    " 'LemasPctPolicOnPatr' 'LemasGangUnitDeploy' 'LemasPctOfficDrugUn'\n",
    " 'PolicBudgPerPop']\n",
    " \n",
    " \n",
    " target_columns:['murders' 'murdPerPop' 'rapes' 'rapesPerPop' 'robberies'\n",
    " 'robbbPerPop' 'assaults' 'assaultPerPop' 'burglaries' 'burglPerPop'\n",
    " 'larcenies' 'larcPerPop' 'autoTheft' 'autoTheftPerPop' 'arsons'\n",
    " 'arsonsPerPop' 'ViolentCrimesPerPop' 'nonViolPerPop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crime_dataset():\n",
    "    import pandas as pd\n",
    "    # Communities and Crime dataset for regression\n",
    "    # source:\n",
    "    # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "    df = pd.read_table('CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "    \n",
    "    # drop columns for city, state, etc. plus rows with values = na\n",
    "    crime = df.drop(df.columns[[0,1,2,3,4]],axis=1).dropna()\n",
    "    \n",
    "    # n columns based on the index\n",
    "#     X_crime = crime.iloc[:,range(0,10)]\n",
    "    \n",
    "    # all predictive columns\n",
    "#     X_crime = crime.iloc[:,range(0,124)]\n",
    "\n",
    "    # select columns from a list\n",
    "    X_crime = crime[['PctPopUnderPov','racepctblack','racePctWhite','racePctAsian',\n",
    " 'racePctHisp','population','medIncome','PctKidsBornNeverMar']]\n",
    "    \n",
    "    # select just one column, will need to do a reshape\n",
    "#     X_crime = crime['perCapInc'].values.reshape(-1,1)\n",
    "    \n",
    "    #your exercise here\n",
    "#     X_crime = crime[[]]\n",
    "\n",
    "    # select any one column from the target columns\n",
    "    y_crime = crime['burglPerPop']\n",
    "\n",
    "    return (X_crime,y_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PctPopUnderPov  racepctblack  racePctWhite  racePctAsian  racePctHisp  \\\n",
      "9            28.68         23.14         67.60          0.92        16.35   \n",
      "13           27.71         53.52         45.65          0.49         0.43   \n",
      "17           14.37          1.30         74.02         14.14        20.96   \n",
      "19            8.21          8.41         82.64          3.92         8.91   \n",
      "21           19.29         28.71         52.26          7.00        24.36   \n",
      "\n",
      "    population  medIncome  PctKidsBornNeverMar  \n",
      "9       103590      17852                 4.71  \n",
      "13       57140      19143                 8.51  \n",
      "17      180038      34372                 2.62  \n",
      "19      261721      35048                 1.89  \n",
      "21     7322564      29823                10.50  \n",
      "9     2221.81\n",
      "13    2987.92\n",
      "17     889.74\n",
      "19    1360.48\n",
      "21    1355.37\n",
      "Name: burglPerPop, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def printDataSet():\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    print(X_crime.head())\n",
    "    print(y_crime.head())\n",
    "    \n",
    "printDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  ['PctPopUnderPov' 'racepctblack' 'racePctWhite' 'racePctAsian'\n",
      " 'racePctHisp' 'population' 'medIncome' 'PctKidsBornNeverMar'] \n",
      "y:  burglPerPop\n"
     ]
    }
   ],
   "source": [
    "def returnColumnNames():\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    print('X: ', X_crime.columns.values, '\\ny: ',y_crime.name)\n",
    "    \n",
    "returnColumnNames()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get a baseset with a dummy regressor\n",
    "\n",
    "def ex0():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "    import numpy as np\n",
    "    from sklearn.dummy import DummyRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    dummy = DummyRegressor().fit(X_train, y_train)\n",
    "\n",
    "    #linreg.score gives the R2 score\n",
    "    return (dummy.score(X_train, y_train),dummy.score(X_test, y_test))\n",
    "\n",
    "ex0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1886.33127843 [  4.55322835e+01   2.83374968e+01   2.51745431e+01   2.94384390e+01\n",
      "   9.37994156e+00  -7.06405361e-05  -1.31012292e-03   5.17937046e+01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.52839388693877787, 0.46117508270956642)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform linear regression\n",
    "\n",
    "def ex1():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    print(linreg.intercept_, linreg.coef_)\n",
    "    #linreg.score gives the R2 score\n",
    "    return (linreg.score(X_train, y_train),linreg.score(X_test, y_test))\n",
    "\n",
    "ex1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35840.1202635 [  0.00000000e+00  -1.74274810e+02  -1.30821976e+03  -7.47557786e+02\n",
      "  -9.81762731e+02  -3.55618574e+02   1.33072569e-02   1.79879473e-02\n",
      "   2.43193308e+03   3.38931078e+00   2.81144520e+00   7.62675910e-01\n",
      "   4.12494126e+00   3.06185843e-01  -2.17530795e-04   4.34494505e-03\n",
      "  -1.85270830e+01   8.31667139e+00   1.23425248e+01   1.53961725e+01\n",
      "   5.31716212e+00  -1.79995091e-05   2.30570439e-03  -1.37266617e+01\n",
      "   3.80475827e+00   9.83465063e+00   3.60234178e+00  -7.08749504e-05\n",
      "   1.74608142e-04  -1.49628110e+01   5.90946226e+00   3.41411406e+00\n",
      "  -9.03857450e-05   5.47703119e-04  -1.94718113e+01   5.40537075e-01\n",
      "   5.32765171e-06   3.45153925e-04  -4.56880684e+00   6.59520412e-11\n",
      "  -1.14220776e-07  -1.50309660e-04  -2.87074771e-07  -2.00111468e-02\n",
      "  -2.89685503e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.64402021775553797, 0.35095535624577934)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform polynomial regression of degree 2, which in theory should give us better score\n",
    "# reality was different, we are now overfitting\n",
    "def ex1a():\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    \n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X_crime)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "    print(linreg.intercept_, linreg.coef_)\n",
    "    return (linreg.score(X_train, y_train),linreg.score(X_test, y_test))\n",
    "\n",
    "ex1a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-769.157517483 [  0.00000000e+00  -4.67832943e-01  -6.36656400e+01   1.26144423e+01\n",
      "   2.16961050e+01  -2.01098110e+00   1.77059880e-02  -6.99181699e-02\n",
      "   1.53453337e+01   3.19051570e+00  -1.02334588e-01  -1.04956558e+00\n",
      "   3.37520313e+00  -7.25001928e-02  -2.20425955e-04   4.22747613e-03\n",
      "  -1.28228154e+01   1.80086931e-01   7.92736071e-02  -8.06321243e-02\n",
      "  -2.48953029e-01  -5.03825457e-05   2.42378014e-03   6.52240359e+00\n",
      "  -8.40211693e-02  -7.08111349e-01  -2.59395626e-01  -1.11553820e-04\n",
      "   9.11148290e-04   7.55100129e+00  -3.42757522e-01  -2.87507161e+00\n",
      "  -1.32861854e-04   1.79520323e-03   5.37103093e+00  -4.50682520e-01\n",
      "  -9.48502481e-06   1.70416619e-03   4.23441559e+00   6.85973139e-11\n",
      "  -1.22934854e-07  -1.88329228e-04  -2.06720563e-07  -1.78636799e-02\n",
      "   2.81393171e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.62850359425733404, 0.45921137323878652)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid the problems of overfitting polynomial regression of higher degrees, \n",
    "# we put a penalty on the coefficients that are large.\n",
    "# We use ridge regression which is a type of regularized linear regression \n",
    "# that uses an alpha parameter that helps to avoid overfitting: \n",
    "def ex2():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    \n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X_crime)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "    print(linridge.intercept_, linridge.coef_)\n",
    "    return (linridge.score(X_train, y_train),linridge.score(X_test, y_test))\n",
    "\n",
    "ex2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157.62257371 [   0.          230.10674632  108.41892359  -88.91363839  -11.69144233\n",
      "   33.65102139  -19.47589015 -110.50852642  139.50283076  189.17767034\n",
      "  120.07375505  184.04406954   23.87405012   36.07527214  -17.3517295\n",
      "  200.13584963  150.50592879   58.89728157  219.44826985   31.89540054\n",
      "   10.52857745  -17.89714567   53.77918542   79.46575624 -136.69159242\n",
      "  -35.52776642   60.45256812  -15.78511387  -95.64399608  198.08992462\n",
      "   -3.91848733  -18.32616216  -30.87700098  -34.60498567   24.20318045\n",
      "    6.0289835   -27.44920444    2.67184648   28.46335601  -26.51745371\n",
      "  -24.28728172  -22.59792134  -74.62343979   65.15994178   84.48159278]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.50677082269673024, 0.47856555005117107)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when features vary wildly, \n",
    "# e.g when calculating the price of the house: \n",
    "# the square footage is in the thousands\n",
    "# and number of bedrooms is in the single digits, it's best to normalize the data \n",
    "# MinMaxScaler will convert values to between 0 and 1 (for all positive values) \n",
    "# or -.5 and .5 (for positive and negative values)\n",
    "# make sure there are no outliers\n",
    "def ex2a():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "#     both training set and testing set need to be scaled\n",
    "#     first we do a fit on the training data: fit calculates the min and max on the data\n",
    "#     then we do a transform on the training and test data with those min and max calculated on the fit\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "    print(linridge.intercept_, linridge.coef_)\n",
    "    return (linridge.score(X_train_scaled, y_train),linridge.score(X_test_scaled, y_test))\n",
    "\n",
    "ex2a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise: use min max scaler and ridge regressor with alpha values in \n",
    "# [0.1, 1, 10, 20, 50, 100, 1000] with a polynomial of degree 3\n",
    "# notice that increasing alpha reduces overfitting\n",
    "def ex3():\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\")\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Ridge\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=3).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for this_alpha in [0.1, 1, 10, 20, 50, 100, 1000]:\n",
    "        linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "        r2_train = linridge.score(X_train_scaled, y_train)\n",
    "        r2_test = linridge.score(X_test_scaled, y_test)\n",
    "        print('Alpha = {:.2f}\\nr-squared training: {:.2f}, r-squared test: {:.2f}'\n",
    "             .format(this_alpha, r2_train, r2_test))\n",
    "\n",
    "ex3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "# another way of doing regularization is using the Lasso Regression, which also penalizes \n",
    "# the coeficients when doing the regression\n",
    "# when to use Lasso vs Ridge?\n",
    "# find which features have the most effect (use DecisionTreeRegressor to find out)\n",
    "# When you have many small/medium sized effects: use Ridge\n",
    "# When you have a few variables with medium/large effects: use Lasso\n",
    "#\n",
    "# exercise: do a Lasso regression for alpha in [0.1, 0.5, 1, 2, 3, 5, 10, 20, 50,100] \n",
    "# and max_iter = 10000 and polynomial of degree 4\n",
    "# notice that increasing alpha decreases overfitting\n",
    "def ex4():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=4).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for alpha in [0.5, 1, 2, 3, 5,10,20,50,100]:\n",
    "        linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "        r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "        r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "        print('Alpha = {:.2f}\\nr-squared training: {:.2f}, r-squared test: {:.2f}'\n",
    "             .format(alpha, r2_train, r2_test))\n",
    "\n",
    "ex4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.researchgate.net/figure/260283043_fig13_Figure-A15-The-non-linear-SVM-classifier-with-the-kernel-trick\n",
    "def plotSVM():\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('svm.png'))\n",
    "    \n",
    "plotSVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines: these are algorithms that project the data to a different plane before finding a match\n",
    "# C is the penalty parameter, decreasing C increases regularization\n",
    "def ex5():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVR\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    clf = SVR(C=10, kernel='linear').fit(X_train_scaled, y_train)\n",
    "    r2_train = clf.score(X_train_scaled, y_train)\n",
    "    r2_test = clf.score(X_test_scaled, y_test)\n",
    "    print(r2_train,r2_test)\n",
    "\n",
    "ex5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prior example was underfitting, \n",
    "# in order to improve our solution when underfitting in SVRs we increase C (decrease regularization)\n",
    "# We are going to use GridSearchCV to run SVR with different parameters\n",
    "# notice that the higher the parameter C (less normalization), the better the prediction\n",
    "def ex6():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.svm import SVR\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # we use GridSearchCV on the train data to find the best parameters\n",
    "    svr = GridSearchCV(SVR( gamma=.1), cv=5,\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3, 1e4], 'kernel':['rbf','linear','poly']})\n",
    "    \n",
    "    clf = svr.fit(X_train_scaled, y_train)\n",
    "    print(clf.best_params_)\n",
    "    r2_train = clf.score(X_train_scaled, y_train)\n",
    "    r2_test = clf.score(X_test_scaled, y_test)\n",
    "    print(r2_train,r2_test)\n",
    "\n",
    "ex6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use TreeRegressor to identify the features that are most important\n",
    "# change the max_depth to 1 through 4 and see how the importance of each feature changes\n",
    "def ex7():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    depth = 2\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(max_depth=depth).fit(X_train_scaled, y_train)\n",
    "    print(tree.feature_importances_)\n",
    "    returnColumnNames()\n",
    "    return (tree.score(X_train_scaled, y_train),tree.score(X_test_scaled, y_test))\n",
    "\n",
    "ex7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features for predicting burglPerPop used in this exercise were selected to show some of the\n",
    "# algorithms and techniques you can use to solve Regression problems. \n",
    "# They were not chosen to do an accurate prediction\n",
    "# now it's your turn:\n",
    "# try to predict 'ViolentCrimesPerPop' or 'nonViolPerPop' using some of the features available in this dataset\n",
    "# try to get a r2 score > 0.7 for the testing set.\n",
    "# (follow the rules at the top to improve undefitting and avoid overfitting )\n",
    "# pick whatever algorithm you would like, but you must use GridSearchCV to find the optimal one\n",
    "# what are the most important features? (use TreeRegressor)\n",
    "# Note: please don't be offended if a classmate determines that a relationship between race, or other indicator\n",
    "# and crime exists\n",
    "\n",
    "def classExercise():\n",
    "    \n",
    "    return \"solution\"\n",
    "\n",
    "classExercise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
