{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Scoring mechanisms for Classification:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Objectives:\n",
    "\n",
    "- Multiclass Classification\n",
    "- Scoring for Classification\n",
    "- Heat Maps\n",
    "- Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients\n",
    "\n",
    "The data have been organized in two different but related classification tasks.\n",
    "\n",
    "column_3C_weka.csv (file with three class labels)\n",
    "\n",
    "The first task consists in classifying patients as belonging to one out of three categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150 patients).\n",
    "\n",
    "column_2C_weka.csv (file with two class labels)\n",
    "\n",
    "For the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).\n",
    "Content\n",
    "\n",
    "Field Descriptions:\n",
    "\n",
    "Each patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):\n",
    "\n",
    "pelvic incidence\n",
    "pelvic tilt\n",
    "lumbar lordosis angle\n",
    "sacral slope\n",
    "pelvic radius\n",
    "grade of spondylolisthesis\n",
    "\n",
    "Acknowledgements\n",
    "\n",
    "The original dataset was downloaded from UCI ML repository:\n",
    "\n",
    "Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science\n",
    "\n",
    "Files were converted to CSV\n",
    "\n",
    "Inspiration\n",
    "\n",
    "Use these biomechanical features to classify patients according to their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readDataWith2Classes():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('column_2C_weka.csv')\n",
    "    y_ortho = df['class'].apply(lambda x: 1 if x in 'Abnormal' else 0)\n",
    "    X_ortho = df.drop(df.columns[[6]],axis=1)\n",
    "    return(X_ortho,y_ortho)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readDataWith3Classes():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('column_3C_weka.csv')\n",
    "    classes = {\"Normal\": 0, \"Spondylolisthesis\" : 1, \"Hernia\": 2}\n",
    "    y_ortho = df['class'].apply(lambda x:classes.get(x))\n",
    "    X_ortho = df.drop(df.columns[[6]],axis=1)\n",
    "    return(X_ortho,y_ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotCounts():\n",
    "    import seaborn as sns\n",
    "    %matplotlib inline\n",
    "    \n",
    "    (X,y) = readDataWith3Classes()\n",
    "    sns.countplot(y,label=\"Count\")\n",
    "    \n",
    "plotCounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following is useful to see if features are correlated\n",
    "# 1 is that they are totally correlated, 0 is that they are not correlated at all, \n",
    "# -1 is that they are negatively correlated (one is the opposite of the other)\n",
    "# in this case they are moderately correlated so you can keep all the features\n",
    "# if features are highly correlated (close to 1), you can remove a feature without \n",
    "# affecting the score\n",
    "# e.g if calculating the price of a house and one feature is 'number of bedrooms', and the other\n",
    "# feature is 'twice the number of bedrooms' they are correlated and the second doesn't add any\n",
    "# extra information so it should be removed\n",
    "def plotHeatMap():\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    #because we are not looking at y we can read either Data2Class or Data3Class\n",
    "    (X,y) = readDataWith2Classes()\n",
    "    corr = X.corr()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 7},\n",
    "           cmap= 'coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "plotHeatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it's a good idea to remove features that are not separatable\n",
    "# in the following graph we can see that features 1, 2 and 3 cannot be separated for either class2 or class3\n",
    "# %matplotlib inline is used to automatically plot without calling plt.show()\n",
    "def plotRelationships():\n",
    "    \n",
    "    import pandas as pd\n",
    "    from pandas.plotting import scatter_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    (X,y) = readDataWith3Classes()\n",
    "    color_function = {0: \"blue\", 1: \"red\", 2: \"green\"}\n",
    "    colors = y.map(lambda x: color_function.get(x))\n",
    "    scatter_matrix(X, c=colors, alpha = 0.5, figsize = (12, 12), diagonal='hist')\n",
    "    plt.show()\n",
    "    \n",
    "plotRelationships()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a decision tree classifier for the data with only two classes, \n",
    "# use a depth = 2 to see which features are more prominently used (the higher the number, the most weight that\n",
    "# feature has)\n",
    "# change the depth from 1 to 4 to see how that changes\n",
    "# as you can see the decision tree agrees with the plot of relationships where the most weight is on the\n",
    "# last two features: 'pelvic_radius' and 'degree_spondylolisthesis'\n",
    "def getDecisionTree():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X,y) = readDataWith2Classes()\n",
    "    depth = 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                       random_state = 0)\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=depth,random_state=0).fit(X_train, y_train)\n",
    "    print(tree.feature_importances_)\n",
    "    print(X.columns)\n",
    "    return (tree.score(X_train, y_train),tree.score(X_test, y_test))\n",
    "\n",
    "getDecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use an SVC and GridSearchCV to do the following:\n",
    "# remove columns [0],[1],[2],[3], [0, 1], [0, 2], [1, 2], and [0,1,2] to see which one gives you the best match\n",
    "# compare it if you leave all columns in\n",
    "# which one do you think is the best? what happens when you take out [3]\n",
    "# use the best one you found for the next three exercises\n",
    "# \n",
    "def SVCGrid():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X,y) = readDataWith2Classes()\n",
    "    \n",
    "    # if you want to include all columns just do    X_clean = X\n",
    "    X_clean = X.drop(X.columns[[0,1,2]],axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, y,\n",
    "                                                       random_state = 0)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1,5,10,20,50]}\n",
    "    clf = GridSearchCV(SVC(), parameters)\n",
    "    \n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_train_scaled,y_train)\n",
    "    svc = clf.best_estimator_.fit(X_train_scaled,y_train)\n",
    "    print(svc)\n",
    "    return (svc.score(X_train_scaled, y_train),svc.score(X_test_scaled, y_test))\n",
    "\n",
    "SVCGrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common scoring for Classification:\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# accuracy: (true pos + true neg)/(true pos + false pos + true neg + false neg) //this is the usual default\n",
    "# precision: (true pos)/(true pos + false pos) // high precision: minimize false positives\n",
    "# e.g. a spam filter needs high precision, so not to flag emails as spam if they are not\n",
    "# \n",
    "# recall: (true pos)/(true pos + false neg) // high recall: minimize false neg\n",
    "# A credit fraud detector needs high recall, so not to let any fraud get through\n",
    "# \n",
    "# f1: F1 = 2 * (precision * recall) / (precision + recall)  //when both precision and recall are important\n",
    "# \n",
    "# roc_auc: Area Under the Receiver Operating Characteristic Curve \n",
    "# roc_auc is used specially for highly unbalanced datasets (one class occurs much more often than the other)\n",
    "# \n",
    "# In this example, what would high precision be? how about high recall? which one do you think\n",
    "# the model should try to maximize in this example\n",
    "# run this example with 'precision' and 'recall' as scoring, which one gives you the best overall score\n",
    "# e.g clf = GridSearchCV(SVC(), parameters,scoring='recall')\n",
    "# can you use the same scoring for a multiclass classification?\n",
    "def SVCGridWithScoring():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X,y) = readDataWith3Classes()\n",
    "    \n",
    "    # if you want to include all columns just do    X_clean = X\n",
    "    X_clean = X.drop(X.columns[[1]],axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clean, y,\n",
    "                                                       random_state = 0)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1,5,10,20,50]}\n",
    "    \n",
    "#     must use scoring=accuracy for multiclass classification\n",
    "    clf = GridSearchCV(SVC(), parameters,scoring='accuracy')\n",
    "    \n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    clf.fit(X_train_scaled,y_train)\n",
    "    svc = clf.best_estimator_.fit(X_train_scaled,y_train)\n",
    "    print(svc)\n",
    "    print(confusion_matrix(y_test, svc.predict(X_test_scaled)))\n",
    "    print(classification_report(y_test, svc.predict(X_test_scaled)))\n",
    "    return (svc.score(X_train_scaled, y_train),svc.score(X_test_scaled, y_test))\n",
    "\n",
    "SVCGridWithScoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a NeuralNetwork and a RandomForest to find your model\n",
    "#  which ones gives you a better approximation?\n",
    "def getNeuralNetwork()\n",
    "\n",
    "\n",
    "\n",
    "getNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a NeuralNetwork and a RandomForest to find your model\n",
    "#  which ones gives you a better approximation?\n",
    "def getRandomForest()\n",
    "\n",
    "\n",
    "\n",
    "getRandomForest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
