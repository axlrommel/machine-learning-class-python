{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics Covered:\n",
    "\n",
    "\n",
    "- Supervised Learning Classification\n",
    "- Scoring: Mean Accuracy ((TP + TN)/Total)\n",
    "- Nearest Neighbor\n",
    "- SVC (Support Vector Machine Classificator)\n",
    "- GridSearch CV\n",
    "- Random Forest\n",
    "- Neural Networks\n",
    "\n",
    "Intuition: \n",
    "A classifier works for one type of problem but not other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet1():\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('scores.txt',header=None)\n",
    "    df.columns = ['test1','test2','pass']\n",
    "\n",
    "    X = df[['test1','test2']]\n",
    "    y = df['pass']\n",
    "    \n",
    "    return(X, y, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet2():\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('transistor.txt',header=None)\n",
    "    df.columns = ['test1','test2','pass']\n",
    "    \n",
    "    X = df[['test1','test2']]\n",
    "    y = df['pass']\n",
    "    return(X, y, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet3():\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('barbecue.txt',header=None)\n",
    "    df.columns = ['test1','test2','pass']\n",
    "    df['test1']=df['test1'].apply(lambda x: (x + 1)/2)\n",
    "    df['test2']=df['test2'].apply(lambda x: (x + 1)/2)\n",
    "    X = df[['test1','test2']]\n",
    "    y = df['pass']\n",
    "    return(X, y, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet4():\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('ex8a.txt',header=None)\n",
    "    df.columns = ['pass','test1','test2']\n",
    "    \n",
    "    X = df[['test1','test2']]\n",
    "    y = df['pass']\n",
    "    return(X, y, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCounts():\n",
    "    # use if not doing plot.show()\n",
    "    %matplotlib inline \n",
    "    import seaborn as sns\n",
    "    \n",
    "    (X, y, df) = getDataSet1()\n",
    "    sns.countplot(y)\n",
    "    \n",
    "plotCounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData():\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    (X,y,df) = getDataSet3()\n",
    "    notPassed = df[df['pass'] == 0]\n",
    "    passed = df[df['pass'] == 1]\n",
    "    \n",
    "    npas, = plt.plot(notPassed['test1'],notPassed['test2'],'go')\n",
    "    pas, = plt.plot(passed['test1'],passed['test2'],'bo')\n",
    "    plt.ylabel('test2')\n",
    "    plt.xlabel('test1')\n",
    "    plt.title('Test 1 vs Test2')\n",
    "    plt.legend([pas, npas], ['Passed', 'Not passed'])\n",
    "    plt.show()\n",
    "\n",
    "plotData()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSolution(df,predict,classifier):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    \n",
    "    gridSize = (df['test1'].max() - df['test1'].min())/100\n",
    "    \n",
    "    x_min, x_max = df['test1'].min() - 3*gridSize, df['test1'].max() + 3*gridSize\n",
    "    y_min, y_max = df['test2'].min() - 3*gridSize, df['test2'].max() + 3*gridSize\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, gridSize),np.arange(y_min, y_max, gridSize))\n",
    "    \n",
    "    Z = predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    notPassed = df[df['pass'] == 0]\n",
    "    passed = df[df['pass'] == 1]\n",
    "    \n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    npas, = plt.plot(notPassed['test1'],notPassed['test2'],'go')\n",
    "    pas, = plt.plot(passed['test1'],passed['test2'],'bo')\n",
    "    plt.ylabel('test2')\n",
    "    plt.xlabel('test1')\n",
    "    plt.title(classifier)\n",
    "    plt.legend([pas, npas], ['Passed', 'Not passed'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default is a stratified solution for dummy classifier\n",
    "def getDummyClassifier():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X,y,df) = getDataSet1()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    dummy = DummyClassifier(strategy='most_frequent').fit(X_train,y_train)\n",
    "#     dummy = DummyClassifier(random_state=0).fit(X_train,y_train)\n",
    "    plotSolution(df,dummy.predict,'Dummy Classifier')\n",
    "    return (dummy.score(X_train, y_train),dummy.score(X_test, y_test))\n",
    "\n",
    "getDummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "def getLogisticRegression():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X,y,df) = getDataSet1()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    logReg = LogisticRegression(random_state=0).fit(X_train,y_train)\n",
    "    plotSolution(df,logReg.predict,'Logistic Regression')\n",
    "    return (logReg.score(X_train, y_train),logReg.score(X_test, y_test))\n",
    "\n",
    "getLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the n_neighbors parameter between 1 and 4 notice the\n",
    "# different graph and results\n",
    "# how many nearest neighbors is best for data set 1, how about for dataset 2?\n",
    "def getKNearestNeighbor():\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    neighbors=4\n",
    "    (X,y,df) = getDataSet1()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    neigh = KNeighborsClassifier(n_neighbors = neighbors).fit(X_train,y_train)\n",
    "    plotSolution(df,neigh.predict,str(neighbors) + ' Nearest Neighbors')\n",
    "    return (neigh.score(X_train, y_train),neigh.score(X_test, y_test))\n",
    "\n",
    "getKNearestNeighbor()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change depth from 1 to 10, what do you notice?\n",
    "def getDecisionTree():\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    depth = 10\n",
    "    (X,y,df) = getDataSet1()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    tree = DecisionTreeClassifier(max_depth = depth, random_state=0).fit(X_train,y_train)\n",
    "    plotSolution(df,tree.predict,'Decision Tree, depth: ' + str(depth))\n",
    "    return (tree.score(X_train, y_train),tree.score(X_test, y_test))\n",
    "\n",
    "getDecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSVC():\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC \n",
    "    \n",
    "    kernelToUse = 'rbf' #try linear as well\n",
    "    (X,y,df) = getDataSet1()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    svc = SVC(kernel = kernelToUse).fit(X_train,y_train)\n",
    "    plotSolution(df,svc.predict, 'SVC, kernel ' + kernelToUse)\n",
    "    return (svc.score(X_train, y_train),svc.score(X_test, y_test))\n",
    "\n",
    "getSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSVCGridSearch():\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    (X,y,df) = getDataSet1()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1,5,10,20,50,100]}\n",
    "    clf = GridSearchCV(SVC(), parameters)\n",
    "    clf.fit(X_train,y_train)\n",
    "    svc = clf.best_estimator_.fit(X_train,y_train)\n",
    "    plotSolution(df,svc.predict,'SVC ' + str(clf.best_params_))\n",
    "    print(svc)\n",
    "    return (svc.score(X_train, y_train),svc.score(X_test, y_test))\n",
    "\n",
    "getSVCGridSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with your best nearest neighbor for dataset1 calculate the probability that \n",
    "# a student that gets 55 and 55, and 60 and 60 will pass, how about 65 and 65? \n",
    "# 70 and 70?\n",
    "# SVC cannot give you probability, will only give you the prediction, so for the best SVC \n",
    "# tell me if that student is going to pass or not for the same grades above\n",
    "def calculateProb():\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    predictArray = [[50,50],[55,55],[60,60],[65,65]]\n",
    "    neighbors=3\n",
    "    (X,y,df) = getDataSet1()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    \n",
    "    neigh = KNeighborsClassifier(n_neighbors = neighbors).fit(X_train,y_train)\n",
    "    \n",
    "    svc = SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False).fit(X_train,y_train)\n",
    "    print(\"Nearest Neighbor: \\n\", neigh.predict_proba(predictArray))\n",
    "    print(\"SVC: \\n\", svc.predict(predictArray))\n",
    "calculateProb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest uses a multitude of decision trees\n",
    "def getRandomForest():\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier \n",
    "    \n",
    "    maxDepth = 10\n",
    "    (X,y,df) = getDataSet1()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    rfc = RandomForestClassifier(max_depth = maxDepth, random_state=0).fit(X_train,y_train)\n",
    "    plotSolution(df,rfc.predict, 'RandomForestClassifier, max_depth ' + str(maxDepth))\n",
    "    return (rfc.score(X_train, y_train),rfc.score(X_test, y_test))\n",
    "\n",
    "getRandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pyimagesearch.com/wp-content/uploads/2016/08/simple_neural_network_header.jpg?width=600\n",
    "\n",
    "def plotNeuralNetwork():\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('simple_neural_network_header.jpg'))\n",
    "    \n",
    "plotNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeuralNet():\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neural_network import MLPClassifier \n",
    "    \n",
    "    hiddenLayers = 100\n",
    "    maxIter = 2000\n",
    "    (X,y,df) = getDataSet1()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "    nn = MLPClassifier(hidden_layer_sizes = hiddenLayers, max_iter = maxIter, random_state=0).fit(X_train,y_train)\n",
    "    plotSolution(df,nn.predict, 'MLPClassifier, hidden_layer_sizes ' + str(hiddenLayers))\n",
    "    return (nn.score(X_train, y_train),nn.score(X_test, y_test))\n",
    "\n",
    "getNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
