{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics covered:\n",
    "- LinearRegression\n",
    "- PolynomialFeatures\n",
    "- Ridge\n",
    "- MinMaxScaler\n",
    "- Lasso\n",
    "- SVR\n",
    "- GridSearchCV\n",
    "- Regularization\n",
    "- DecisionTreeRegressor\n",
    "- Tips for when having overfitting and underfitting:\n",
    "\n",
    "GridSearchCV:\n",
    "\n",
    "\n",
    "When having overfitting (high variance), the model is not a good predictor:\n",
    "- get more training examples\n",
    "- reduce the number of features (but don't use PCA!)\n",
    "- adding features is likely going to make things worse\n",
    "- Increase regularization parameter (in e.g. Lasso and Ridge is called alpha parameter)\n",
    "- If using SVMs (SVR or SVC) decrease C (because C = 1/(regularization parameter))\n",
    "\n",
    "When having underfitting (high bias), the model is too simple:\n",
    "- get more features\n",
    "- add polynomial terms\n",
    "- getting more training examples is not going to help\n",
    "- Decrease regularization parameter (in e.g. Lasso and Ridge is called alpha parameter)\n",
    "- If using SVMs (SVR or SVC) increase C (because C = 1/(regularization parameter))\n",
    "\n",
    "What is regularization used for?\n",
    "- In lay terms, we use regularization when we have many features, but we want to reduce the magnitude of their influence in the final model.\n",
    "- The other option is just dropping those features altogether.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the crime dataset found here\n",
    "https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "\n",
    "below are the columns from the dataset:\n",
    "\n",
    "predictive_columns = ['population' 'householdsize' 'racepctblack' 'racePctWhite' 'racePctAsian'\n",
    " 'racePctHisp' 'agePct12t21' 'agePct12t29' 'agePct16t24' 'agePct65up'\n",
    " 'numbUrban' 'pctUrban' 'medIncome' 'pctWWage' 'pctWFarmSelf' 'pctWInvInc'\n",
    " 'pctWSocSec' 'pctWPubAsst' 'pctWRetire' 'medFamInc' 'perCapInc'\n",
    " 'whitePerCap' 'blackPerCap' 'indianPerCap' 'AsianPerCap' 'OtherPerCap'\n",
    " 'HispPerCap' 'NumUnderPov' 'PctPopUnderPov' 'PctLess9thGrade'\n",
    " 'PctNotHSGrad' 'PctBSorMore' 'PctUnemployed' 'PctEmploy' 'PctEmplManu'\n",
    " 'PctEmplProfServ' 'PctOccupManu' 'PctOccupMgmtProf' 'MalePctDivorce'\n",
    " 'MalePctNevMarr' 'FemalePctDiv' 'TotalPctDiv' 'PersPerFam' 'PctFam2Par'\n",
    " 'PctKids2Par' 'PctYoungKids2Par' 'PctTeen2Par' 'PctWorkMomYoungKids'\n",
    " 'PctWorkMom' 'NumKidsBornNeverMar' 'PctKidsBornNeverMar' 'NumImmig'\n",
    " 'PctImmigRecent' 'PctImmigRec5' 'PctImmigRec8' 'PctImmigRec10'\n",
    " 'PctRecentImmig' 'PctRecImmig5' 'PctRecImmig8' 'PctRecImmig10'\n",
    " 'PctSpeakEnglOnly' 'PctNotSpeakEnglWell' 'PctLargHouseFam'\n",
    " 'PctLargHouseOccup' 'PersPerOccupHous' 'PersPerOwnOccHous'\n",
    " 'PersPerRentOccHous' 'PctPersOwnOccup' 'PctPersDenseHous' 'PctHousLess3BR'\n",
    " 'MedNumBR' 'HousVacant' 'PctHousOccup' 'PctHousOwnOcc' 'PctVacantBoarded'\n",
    " 'PctVacMore6Mos' 'MedYrHousBuilt' 'PctHousNoPhone' 'PctWOFullPlumb'\n",
    " 'OwnOccLowQuart' 'OwnOccMedVal' 'OwnOccHiQuart' 'OwnOccQrange' 'RentLowQ'\n",
    " 'RentMedian' 'RentHighQ' 'RentQrange' 'MedRent' 'MedRentPctHousInc'\n",
    " 'MedOwnCostPctInc' 'MedOwnCostPctIncNoMtg' 'NumInShelters' 'NumStreet'\n",
    " 'PctForeignBorn' 'PctBornSameState' 'PctSameHouse85' 'PctSameCity85'\n",
    " 'PctSameState85' 'LemasSwornFT' 'LemasSwFTPerPop' 'LemasSwFTFieldOps'\n",
    " 'LemasSwFTFieldPerPop' 'LemasTotalReq' 'LemasTotReqPerPop'\n",
    " 'PolicReqPerOffic' 'PolicPerPop' 'RacialMatchCommPol' 'PctPolicWhite'\n",
    " 'PctPolicBlack' 'PctPolicHisp' 'PctPolicAsian' 'PctPolicMinor'\n",
    " 'OfficAssgnDrugUnits' 'NumKindsDrugsSeiz' 'PolicAveOTWorked' 'LandArea'\n",
    " 'PopDens' 'PctUsePubTrans' 'PolicCars' 'PolicOperBudg'\n",
    " 'LemasPctPolicOnPatr' 'LemasGangUnitDeploy' 'LemasPctOfficDrugUn'\n",
    " 'PolicBudgPerPop']\n",
    " \n",
    " \n",
    " target_columns:['murders' 'murdPerPop' 'rapes' 'rapesPerPop' 'robberies'\n",
    " 'robbbPerPop' 'assaults' 'assaultPerPop' 'burglaries' 'burglPerPop'\n",
    " 'larcenies' 'larcPerPop' 'autoTheft' 'autoTheftPerPop' 'arsons'\n",
    " 'arsonsPerPop' 'ViolentCrimesPerPop' 'nonViolPerPop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crime_dataset():\n",
    "    import pandas as pd\n",
    "    # Communities and Crime dataset for regression\n",
    "    # source:\n",
    "    # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "    df = pd.read_table('CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "    \n",
    "    # drop columns for city, state, etc. plus rows with values = na\n",
    "    crime = df.drop(df.columns[[0,1,2,3,4]],axis=1).dropna()\n",
    "    \n",
    "    # n columns based on the index\n",
    "#     X_crime = crime.iloc[:,range(0,10)]\n",
    "    \n",
    "    # all predictive columns\n",
    "#     X_crime = crime.iloc[:,range(0,124)]\n",
    "\n",
    "    # select columns from a list\n",
    "    X_crime = crime[['PctPopUnderPov','racepctblack','racePctWhite','racePctAsian',\n",
    " 'racePctHisp','population','medIncome','PctKidsBornNeverMar']]\n",
    "    \n",
    "    # select just one column, will need to do a reshape\n",
    "#     X_crime = crime['perCapInc'].values.reshape(-1,1)\n",
    "    \n",
    "    #your exercise here\n",
    "#     X_crime = crime[[]]\n",
    "\n",
    "    # select any one column from the target columns\n",
    "    y_crime = crime['burglPerPop']\n",
    "\n",
    "    return (X_crime,y_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PctPopUnderPov  racepctblack  racePctWhite  racePctAsian  racePctHisp  \\\n",
      "9            28.68         23.14         67.60          0.92        16.35   \n",
      "13           27.71         53.52         45.65          0.49         0.43   \n",
      "17           14.37          1.30         74.02         14.14        20.96   \n",
      "19            8.21          8.41         82.64          3.92         8.91   \n",
      "21           19.29         28.71         52.26          7.00        24.36   \n",
      "\n",
      "    population  medIncome  PctKidsBornNeverMar  \n",
      "9       103590      17852                 4.71  \n",
      "13       57140      19143                 8.51  \n",
      "17      180038      34372                 2.62  \n",
      "19      261721      35048                 1.89  \n",
      "21     7322564      29823                10.50  \n",
      "9     2221.81\n",
      "13    2987.92\n",
      "17     889.74\n",
      "19    1360.48\n",
      "21    1355.37\n",
      "Name: burglPerPop, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def printDataSet():\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    print(X_crime.head())\n",
    "    print(y_crime.head())\n",
    "    \n",
    "printDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  ['PctPopUnderPov' 'racepctblack' 'racePctWhite' 'racePctAsian'\n",
      " 'racePctHisp' 'population' 'medIncome' 'PctKidsBornNeverMar'] \n",
      "y:  burglPerPop\n"
     ]
    }
   ],
   "source": [
    "def returnColumnNames():\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    print('X: ', X_crime.columns.values, '\\ny: ',y_crime.name)\n",
    "    \n",
    "returnColumnNames()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -0.00012124930948465007)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's get a baseset with a dummy regressor\n",
    "\n",
    "def ex0():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "    import numpy as np\n",
    "    from sklearn.dummy import DummyRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    dummy = DummyRegressor().fit(X_train, y_train)\n",
    "\n",
    "    #linreg.score gives the R2 score\n",
    "    return (dummy.score(X_train, y_train),dummy.score(X_test, y_test))\n",
    "\n",
    "ex0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.52839388693877787, 0.46117508270956642)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform linear regression\n",
    "\n",
    "def ex1():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    #linreg.score gives the R2 score\n",
    "    return (linreg.score(X_train, y_train),linreg.score(X_test, y_test))\n",
    "\n",
    "ex1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.64402021775553608, 0.35095535624578356)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform polynomial regression of degree 2, which in theory should give us better score\n",
    "# reality was different, we are now overfitting\n",
    "def ex1a():\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    \n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X_crime)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    return (linreg.score(X_train, y_train),linreg.score(X_test, y_test))\n",
    "\n",
    "ex1a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.62850359425733404, 0.45921137323878652)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid the problems of overfitting polynomial regression of higher degrees, \n",
    "# we put a penalty on the coefficients that are large.\n",
    "# We use ridge regression which is a type of regularized linear regression \n",
    "# that uses an alpha parameter (sometimes called lambda parameter) \n",
    "# to penalize for large coefs theta (to avoid overfitting): \n",
    "def ex2():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    \n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X_crime)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "    return (linridge.score(X_train, y_train),linridge.score(X_test, y_test))\n",
    "\n",
    "ex2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.53526522806995325, 0.48953144501529067)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when features vary wildly, e.g when calculating the price of the house: \n",
    "# the square footage is in the thousands\n",
    "# and number of bedrooms is in the single digits, it's best to normalize the data \n",
    "# to values between 0 and 1 or -1 and 1, you use MinMaxScaler on most \n",
    "# occasions - make sure there are no outliers\n",
    "def ex2a():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=3).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "    return (linridge.score(X_train_scaled, y_train),linridge.score(X_test_scaled, y_test))\n",
    "\n",
    "ex2a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.10\n",
      "r-squared training: 0.64, r-squared test: 0.45\n",
      "Alpha = 1.00\n",
      "r-squared training: 0.60, r-squared test: 0.53\n",
      "Alpha = 10.00\n",
      "r-squared training: 0.55, r-squared test: 0.50\n",
      "Alpha = 20.00\n",
      "r-squared training: 0.54, r-squared test: 0.49\n",
      "Alpha = 50.00\n",
      "r-squared training: 0.51, r-squared test: 0.48\n",
      "Alpha = 100.00\n",
      "r-squared training: 0.47, r-squared test: 0.45\n",
      "Alpha = 1000.00\n",
      "r-squared training: 0.18, r-squared test: 0.17\n"
     ]
    }
   ],
   "source": [
    "# use min max scaler and ridge regressor with alpha values in \n",
    "# [0.1, 1, 10, 20, 50, 100, 1000] with a polynomial of degree 3\n",
    "# notice that increasing alpha reduces overfitting\n",
    "def ex3():\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\")\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Ridge\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=3).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for this_alpha in [0.1, 1, 10, 20, 50, 100, 1000]:\n",
    "        linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "        r2_train = linridge.score(X_train_scaled, y_train)\n",
    "        r2_test = linridge.score(X_test_scaled, y_test)\n",
    "        print('Alpha = {:.2f}\\nr-squared training: {:.2f}, r-squared test: {:.2f}'\n",
    "             .format(this_alpha, r2_train, r2_test))\n",
    "\n",
    "ex3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.50\n",
      "r-squared training: 0.65, r-squared test: 0.42\n",
      "Alpha = 1.00\n",
      "r-squared training: 0.62, r-squared test: 0.49\n",
      "Alpha = 2.00\n",
      "r-squared training: 0.60, r-squared test: 0.50\n",
      "Alpha = 3.00\n",
      "r-squared training: 0.58, r-squared test: 0.50\n",
      "Alpha = 5.00\n",
      "r-squared training: 0.54, r-squared test: 0.46\n",
      "Alpha = 10.00\n",
      "r-squared training: 0.52, r-squared test: 0.46\n",
      "Alpha = 20.00\n",
      "r-squared training: 0.50, r-squared test: 0.45\n",
      "Alpha = 50.00\n",
      "r-squared training: 0.40, r-squared test: 0.36\n",
      "Alpha = 100.00\n",
      "r-squared training: 0.11, r-squared test: 0.09\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "# another way of doing regularization is using the Lasso Regression, which also penalizes \n",
    "# the coeficients when doing the regression\n",
    "# when to use Lasso vs Ridge?\n",
    "# find which features have the most effect (use for example DecisionTreeRegressor explained below)\n",
    "# When you have many small/medium sized effects: use Ridge\n",
    "# When you have a few variables with medium/large effects: use Lasso\n",
    "#\n",
    "# do a Lasso regression for alpha in [0.1, 0.5, 1, 2, 3, 5, 10, 20, 50,100] \n",
    "# and max_iter = 10000 and polynomial of degree 4\n",
    "# notice that increasing alpha decreases overfitting\n",
    "def ex4():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=4).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for alpha in [0.5, 1, 2, 3, 5,10,20,50,100]:\n",
    "        linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "        r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "        r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "        print('Alpha = {:.2f}\\nr-squared training: {:.2f}, r-squared test: {:.2f}'\n",
    "             .format(alpha, r2_train, r2_test))\n",
    "\n",
    "ex4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.228880840814 0.223396935623\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines: these are algorithms that transform the data before finding a match\n",
    "# C is the penalty parameter\n",
    "# decreasing C increases regularization\n",
    "# let's just get \n",
    "def ex5():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVR\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    clf = SVR(C=10, kernel='linear').fit(X_train_scaled, y_train)\n",
    "    r2_train = clf.score(X_train_scaled, y_train)\n",
    "    r2_test = clf.score(X_test_scaled, y_test)\n",
    "    print(r2_train,r2_test)\n",
    "\n",
    "ex5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10000.0, 'kernel': 'rbf'}\n",
      "0.53540402058 0.500005846702\n"
     ]
    }
   ],
   "source": [
    "# The prior example was underfitting, \n",
    "# in order to improve our solution when underfitting in SVRs we increase C (decrease regularization)\n",
    "# We are going to use GridSearchCV to run SVR with different parameters\n",
    "# notice that the best result is going to have the highest parameter C\n",
    "def ex6():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.svm import SVR\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # we use GridSearchCV on the train data to find the best parameters\n",
    "    svr = GridSearchCV(SVR( gamma=0.1), cv=5,\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3, 1e4], 'kernel':['rbf','linear','poly']})\n",
    "    \n",
    "    clf = svr.fit(X_train_scaled, y_train)\n",
    "    print(clf.best_params_)\n",
    "    r2_train = clf.score(X_train_scaled, y_train)\n",
    "    r2_test = clf.score(X_test_scaled, y_test)\n",
    "    print(r2_train,r2_test)\n",
    "\n",
    "ex6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23672563  0.          0.          0.          0.          0.\n",
      "  0.05450671  0.70876767]\n",
      "X:  ['PctPopUnderPov' 'racepctblack' 'racePctWhite' 'racePctAsian'\n",
      " 'racePctHisp' 'population' 'medIncome' 'PctKidsBornNeverMar'] \n",
      "y:  burglPerPop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.51961245661659561, 0.38031607613654439)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use TreeRegressor to identify the features that are most important\n",
    "# change the max_depth to 1 through 4 and see how the important features change\n",
    "def ex7():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    depth = 2\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(max_depth=depth).fit(X_train_scaled, y_train)\n",
    "    print(tree.feature_importances_)\n",
    "    returnColumnNames()\n",
    "    return (tree.score(X_train_scaled, y_train),tree.score(X_test_scaled, y_test))\n",
    "\n",
    "ex7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features used for this exercise for predicting burglPerPop were selected to show some of the\n",
    "# algorithms you can use to solve Regression problems.\n",
    "# They are not necessarily the most appropriate to do an accurate prediction for this specific problem\n",
    "# now it's your turn:\n",
    "# try to predict 'ViolentCrimesPerPop' or 'nonViolPerPop'\n",
    "# with a r2 score of over 0.7 without overfitting\n",
    "# what are the most important features? (use TreeRegressor)\n",
    "\n",
    "def classExercise():\n",
    "    \n",
    "    return \"solution\"\n",
    "\n",
    "classExercise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
