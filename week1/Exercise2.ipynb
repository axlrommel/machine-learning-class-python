{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the crime dataset found here\n",
    "https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "\n",
    "below are the columns from the dataset:\n",
    "\n",
    "predictive_columns = ['population' 'householdsize' 'racepctblack' 'racePctWhite' 'racePctAsian'\n",
    " 'racePctHisp' 'agePct12t21' 'agePct12t29' 'agePct16t24' 'agePct65up'\n",
    " 'numbUrban' 'pctUrban' 'medIncome' 'pctWWage' 'pctWFarmSelf' 'pctWInvInc'\n",
    " 'pctWSocSec' 'pctWPubAsst' 'pctWRetire' 'medFamInc' 'perCapInc'\n",
    " 'whitePerCap' 'blackPerCap' 'indianPerCap' 'AsianPerCap' 'OtherPerCap'\n",
    " 'HispPerCap' 'NumUnderPov' 'PctPopUnderPov' 'PctLess9thGrade'\n",
    " 'PctNotHSGrad' 'PctBSorMore' 'PctUnemployed' 'PctEmploy' 'PctEmplManu'\n",
    " 'PctEmplProfServ' 'PctOccupManu' 'PctOccupMgmtProf' 'MalePctDivorce'\n",
    " 'MalePctNevMarr' 'FemalePctDiv' 'TotalPctDiv' 'PersPerFam' 'PctFam2Par'\n",
    " 'PctKids2Par' 'PctYoungKids2Par' 'PctTeen2Par' 'PctWorkMomYoungKids'\n",
    " 'PctWorkMom' 'NumKidsBornNeverMar' 'PctKidsBornNeverMar' 'NumImmig'\n",
    " 'PctImmigRecent' 'PctImmigRec5' 'PctImmigRec8' 'PctImmigRec10'\n",
    " 'PctRecentImmig' 'PctRecImmig5' 'PctRecImmig8' 'PctRecImmig10'\n",
    " 'PctSpeakEnglOnly' 'PctNotSpeakEnglWell' 'PctLargHouseFam'\n",
    " 'PctLargHouseOccup' 'PersPerOccupHous' 'PersPerOwnOccHous'\n",
    " 'PersPerRentOccHous' 'PctPersOwnOccup' 'PctPersDenseHous' 'PctHousLess3BR'\n",
    " 'MedNumBR' 'HousVacant' 'PctHousOccup' 'PctHousOwnOcc' 'PctVacantBoarded'\n",
    " 'PctVacMore6Mos' 'MedYrHousBuilt' 'PctHousNoPhone' 'PctWOFullPlumb'\n",
    " 'OwnOccLowQuart' 'OwnOccMedVal' 'OwnOccHiQuart' 'OwnOccQrange' 'RentLowQ'\n",
    " 'RentMedian' 'RentHighQ' 'RentQrange' 'MedRent' 'MedRentPctHousInc'\n",
    " 'MedOwnCostPctInc' 'MedOwnCostPctIncNoMtg' 'NumInShelters' 'NumStreet'\n",
    " 'PctForeignBorn' 'PctBornSameState' 'PctSameHouse85' 'PctSameCity85'\n",
    " 'PctSameState85' 'LemasSwornFT' 'LemasSwFTPerPop' 'LemasSwFTFieldOps'\n",
    " 'LemasSwFTFieldPerPop' 'LemasTotalReq' 'LemasTotReqPerPop'\n",
    " 'PolicReqPerOffic' 'PolicPerPop' 'RacialMatchCommPol' 'PctPolicWhite'\n",
    " 'PctPolicBlack' 'PctPolicHisp' 'PctPolicAsian' 'PctPolicMinor'\n",
    " 'OfficAssgnDrugUnits' 'NumKindsDrugsSeiz' 'PolicAveOTWorked' 'LandArea'\n",
    " 'PopDens' 'PctUsePubTrans' 'PolicCars' 'PolicOperBudg'\n",
    " 'LemasPctPolicOnPatr' 'LemasGangUnitDeploy' 'LemasPctOfficDrugUn'\n",
    " 'PolicBudgPerPop']\n",
    " \n",
    " \n",
    " target_columns:['murders' 'murdPerPop' 'rapes' 'rapesPerPop' 'robberies'\n",
    " 'robbbPerPop' 'assaults' 'assaultPerPop' 'burglaries' 'burglPerPop'\n",
    " 'larcenies' 'larcPerPop' 'autoTheft' 'autoTheftPerPop' 'arsons'\n",
    " 'arsonsPerPop' 'ViolentCrimesPerPop' 'nonViolPerPop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crime_dataset():\n",
    "    import pandas as pd\n",
    "    # Communities and Crime dataset for regression\n",
    "    # source:\n",
    "    # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "    df = pd.read_table('CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "    \n",
    "    # drop columns for city, state, etc. plus rows with values = na\n",
    "    crime = df.drop(df.columns[[0,1,2,3,4]],axis=1).dropna()\n",
    "    \n",
    "    # n columns based on the index\n",
    "    X_crime = crime.iloc[:,range(0,10)]\n",
    "    \n",
    "    # all predictive columns\n",
    "#     X_crime = crime.iloc[:,range(0,124)]\n",
    "\n",
    "    # select columns from a list\n",
    "#     X_crime = crime[['householdsize','racepctblack','racePctWhite','racePctAsian','racePctHisp']]\n",
    "    \n",
    "    # select just one column, will need to do a reshape\n",
    "#     X_crime = crime['perCapInc'].values.reshape(-1,1)\n",
    "    \n",
    "    # select any one column from the target columns\n",
    "    y_crime = crime['murdPerPop']\n",
    "\n",
    "    return (X_crime,y_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDataSet():\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    print(X_crime.head())\n",
    "    print(y_crime.head())\n",
    "#     print(X_crime.columns.values)\n",
    "    \n",
    "printDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform linear regression\n",
    "def ex1():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    return (linreg.score(X_train, y_train),linreg.score(X_test, y_test))\n",
    "\n",
    "ex1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform polynomial regression of degree 2\n",
    "def ex1a():\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=2,include_bias=False).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    return (linreg.score(X_train, y_train),linreg.score(X_test, y_test))\n",
    "\n",
    "ex1a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid the problems of overfitting polynomial regression of higher degrees, we put a penalty on the\n",
    "# coefficients that are large.\n",
    "# We use ridge regression which is a type of regularized linear regression that uses an alpha parameter \n",
    "# to penalizes for large coefs theta (to avoid overfitting):\n",
    "# As the magnitudes of the xi parameters increases (the higher the polunomial degree), the penalty increases as well \n",
    "def ex2():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=2,include_bias=False).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "    return (linridge.score(X_train, y_train),linridge.score(X_test, y_test))\n",
    "\n",
    "ex2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when features vary wildly, e.g when calculating the price of the house: the square footage is in the thousands\n",
    "# and number of bedrooms is in the single digits, it's best to normalize the data to values \n",
    "# between 0 and 1 or -1 and 1, you use MinMaxScaler on most occasions - make sure there are no outliers\n",
    "def ex2a():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=2,include_bias=False).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                   random_state = 0)\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "    return (linridge.score(X_train_scaled, y_train),linridge.score(X_test_scaled, y_test))\n",
    "\n",
    "ex2a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use min max scaler and ridge regressor with alpha values in [0, 1, 10, 20, 50, 100, 1000] with a polynomial\n",
    "# of degree 3\n",
    "# find the best alpha\n",
    "def ex3():\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action=\"ignore\", module=\"scipy\")\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Ridge\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=3,include_bias=False).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "        linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "        r2_train = linridge.score(X_train_scaled, y_train)\n",
    "        r2_test = linridge.score(X_test_scaled, y_test)\n",
    "        print('Alpha = {:.2f}\\nr-squared training: {:.2f}, r-squared test: {:.2f}'\n",
    "             .format(this_alpha, r2_train, r2_test))\n",
    "\n",
    "ex3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "# another way of doing regularization is using the Lasso Regression, which also penalizes the coeficients\n",
    "# when doing the regression\n",
    "# Many small/medium sized effects: use Ridge\n",
    "# Only a few variables with medium/large effects: use Lasso\n",
    "# do a lasso regression for alpha in [0.1, 0.5, 1, 2, 3, 5, 10, 20, 50] and max_iter = 10000 and polynomial\n",
    "# of degree 4\n",
    "def ex4():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_poly = PolynomialFeatures(degree=4,include_bias=False).fit_transform(X_crime)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for alpha in [0.1, 0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "        linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "        r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "        r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "        print('Alpha = {:.2f}\\nr-squared training: {:.2f}, r-squared test: {:.2f}'\n",
    "             .format(alpha, r2_train, r2_test))\n",
    "\n",
    "ex4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines transform of the data before finding a match \n",
    "def ex5():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVR\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    clf = SVR(C=10, epsilon=0.2).fit(X_train_scaled, y_train)\n",
    "    r2_train = clf.score(X_train_scaled, y_train)\n",
    "    r2_test = clf.score(X_test_scaled, y_test)\n",
    "    print(r2_train,r2_test)\n",
    "\n",
    "ex5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to run SVR with different parameters \n",
    "def ex6():\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.svm import SVR\n",
    "    import numpy as np\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X_crime,y_crime) = get_crime_dataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                       random_state = 0)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3]})\n",
    "    \n",
    "    clf = svr.fit(X_train_scaled, y_train)\n",
    "    print(clf.best_params_)\n",
    "    r2_train = clf.score(X_train_scaled, y_train)\n",
    "    r2_test = clf.score(X_test_scaled, y_test)\n",
    "    print(r2_train,r2_test)\n",
    "\n",
    "ex6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
