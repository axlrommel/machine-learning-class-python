{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise Objectives:\n",
    "\n",
    "- PCA (Principal Component Analysis)\n",
    "Surprisingly in the real world many features are highly correlated (Andre Ng)\n",
    "PCA uses Singular Value Decomposition, and allows us to reduce the number of features \n",
    "(reduce the dimension of the dataset)\n",
    "In the example below the Breast Cancer Dataset is composed of 30 features. By doing PCA we are able to recreate the dataset with only 16 new features (components) while retaining 99% of the data variance. \n",
    "\n",
    "PCA has two main uses: \n",
    "- If we have a large number of features, PCA allows us to reduce the number of features so we can speed up the learning algorithm\n",
    "- Help us visualize the data\n",
    "\n",
    "PCA shouldn't:\n",
    "- It shouldn't be used to prevent overfitting\n",
    "- It shouldn't be used on your testing set\n",
    "\n",
    "Note: before doing PCA, it's a standard practice to first perform mean normalization (MinMaxScaler) so new features have comparable range of values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast Cancer DataSet:\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "\n",
    "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "All feature values are recoded with four significant digits.\n",
    "\n",
    "Missing attribute values: none\n",
    "\n",
    "Class distribution: 357 benign, 212 malignant\n",
    "\n",
    "kernels (results from other people): https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/kernels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('cancer.txt')\n",
    "    \n",
    "    cancer = df.drop(df.columns[[0,32]],axis=1)\n",
    "    y_cancer = cancer['diagnosis'].apply(lambda x: 0 if x in 'B' else 1)\n",
    "    X_cancer = cancer.drop(cancer.columns[[0]],axis=1)\n",
    "    return(X_cancer, y_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnColumnNames():\n",
    "    (X,y) = readData()\n",
    "    print('X: ', X.columns.values, '\\ny: ',y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree():\n",
    "    \n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X,y) = readData()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                       random_state = 0)\n",
    "    # both training set and testing set need to be scaled\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=2,random_state=0).fit(X_train_scaled, y_train)\n",
    "    print(tree.feature_importances_)\n",
    "    print(returnColumnNames())\n",
    "    return (tree.score(X_train_scaled, y_train),tree.score(X_test_scaled, y_test))\n",
    "\n",
    "decisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTopTwoFeatures():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None # to stop warning\n",
    "    \n",
    "    (X,y) = readData()\n",
    "    df = X[['concave points_mean','area_worst']]\n",
    "    df['diagnosis'] = y\n",
    "    benign = df[df['diagnosis'] == 0]\n",
    "    malign = df[df['diagnosis'] == 1]\n",
    "    \n",
    "    plt.plot(benign['concave points_mean'],benign['area_worst'],'go')\n",
    "    plt.plot(malign['concave points_mean'],malign['area_worst'],'bo')\n",
    "    plt.show()\n",
    "\n",
    "plotTopTwoFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get a good model using the data from PCA and use it with the real data\n",
    "\n",
    "def PCAAnalysis():\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    (X,y) = readData()\n",
    "    \n",
    "    ## use PCA to get a good model: do not use the test set, just the training set to get your model, remember!\n",
    "    # always scale your data for PCA first\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # keep 99% of the variance (99% of the data) this is standard\n",
    "    pca = PCA(svd_solver = 'full', n_components=.99)\n",
    "    \n",
    "    # let's get now a reduced dataset\n",
    "    X_pca = pca.fit(X_scaled).transform(X_scaled)\n",
    "    \n",
    "    # in this case 16 components explain 99% of the data variance\n",
    "    # if you have a million rows: processing 1'000,000x16 is faster than 1'000,000x30 \n",
    "    print(pca.n_components_)\n",
    "    \n",
    "    # plot the first two components from PCA (the two most important of the new dataset)\n",
    "    # notice the plot is similar to the plot of the top two features of the breast cancer data\n",
    "    for i in range(len(y)):\n",
    "        plt.plot(X_pca[i,0],X_pca[i,1],'go') if y[i] == 0 else plt.plot(X_pca[i,0],X_pca[i,1],'bo')\n",
    "    plt.show()\n",
    "    \n",
    "    # we still want to split the data to use GridSearchCV, we will not use the test data for this part\n",
    "    X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y,\n",
    "                                                       random_state = 0)\n",
    "    \n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1,5,10,20,50]}\n",
    "    clf = GridSearchCV(SVC(), parameters)\n",
    "    clf.fit(X_train_pca,y_train_pca)\n",
    "    bestSVCfromPCA = clf.best_estimator_.fit(X_train_pca,y_train_pca)\n",
    "\n",
    "    ## now use the best model found by PCA with the real data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                       random_state = 0)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    svc = bestSVCfromPCA.fit(X_train,y_train)\n",
    "    return (svc.score(X_train, y_train),svc.score(X_test, y_test))\n",
    "    \n",
    "PCAAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
